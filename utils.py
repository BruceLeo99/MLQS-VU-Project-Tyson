# Reuseable functions for data preprocessing and evaluation for our project
# Make sure all packages are imported in your script. This script does not load packages.

import cowsay
cowsay.cow("Feed me some grass! Ik heb honger")

def shuffle_data(feature, label):

    """
    Shuffles a pre-splitted but unshuffled dataframe and returns a new, splitted, and shuffled dataframe
    :param DataFrame feature: unshuffled dataframe of features (x_train or x_test)
    :param DataFrame label: unshuffled dataframe of the label of features (y_train or y_test)
    :return DataFrame shuffled_feature, shuffled_label: shuffled x_train/test and y_train/test
    """
    df_unshuffled = pd.concat([feature, label], axis=1)
    df_shuffled = df_unshuffled.sample(frac=1.0, random_state=50)
    shuffled_feature, shuffled_label = df_shuffled.iloc[:, :-1], df_shuffled.iloc[:, -1]
    return shuffled_feature, shuffled_label


def get_metrics_from_report(report, class_name):

    """
    Retrieves and returns the precision, recall, F1-score of a class (punch type), and
        the overall accuracy of a ML model from the classification report.

    Because the classification report is a string, this function will do
        string formatting to get all of these information.

    :param report: the classification report generated by sklearn
    :param str class_name: the class (punch type) that we are interested in.
    :return tuple(float(precision), float(recall), float(f1), float(accuracy)): a tuple of metrics of the given class

    Example usage:
    Get the evaluation metrics of jab:

    report = sklearn.metrics.classification_report(y_test, y_pred)
    jab_metrics = get_metrics_from_report(report, class_name = 'jab')

    # result:
        jab_metrics[0] -> Precision of jab
        jab_metrics[1] -> Recall of jab
        jab_metrics[2] -> f1 score of jab
        jab_metrics[3] -> The overall accuracy of a ML model
    """

    precision = 0
    recall = 0
    f1 = 0
    accuracy = 0

    report_l1 = report.split("\n")
    report_l1.pop(0)
    report_l1.pop(0)

    report_l2 = []

    for i in report_l1:
        i2 = i.strip()
        report_l2.append(i2.split("       "))

    for i in report_l2:
        if len(i) == 1:
            report_l2.remove(i)

    for idx, j in enumerate(report_l2):

        if j[0] == class_name:
            scores = j[1].split("     ")

            precision = float(scores[0].strip())
            recall = float(scores[1].strip())
            f1 = float(scores[2].strip())

        if j[0] == 'accuracy':
            accuracy = float(report_l2[idx][3].strip())

    return precision, recall, f1, accuracy


def knn_model_pred(k, returns_report=False, print_results=False):
    """
    Trains a knn model at a given k, and makes predictions.
    Assume data have already been preprocessed (splitted, shuffled and normalized).

    Optional (False by default):
    - Returns a classification report if returns_report = True.
    - Prints the evaluation results if print_results = True.
    """

    knn = KNeighborsClassifier(n_neighbors=k)

    knn.fit(x_train_shuffled, y_train_shuffled)

    y_pred = knn.predict(x_test_shuffled)

    print(f"training at {k} neighbors complete!")

    if print_results:
        print("Confusion Matrix:")
        print(confusion_matrix(y_test_shuffled, y_pred))
        print("\nClassification Report:")
        print(classification_report(y_test_shuffled, y_pred))

    if returns_report:
        return classification_report(y_test_shuffled, y_pred)

def random_forest_classifier(n_estimators, returns_report=False, print_results=False):
    """_summary_

    Args:
        nr_estimators (_type_): _description_
        returns_report (bool, optional): _description_. Defaults to False.
        print_results (bool, optional): _description_. Defaults to False.
    """

    rfc = RandomForestClassifier(n_estimators=n_estimators, random_state=42)
    
    rfc.fit(X_train, y_train)
    
    print(f"training at {n_estimators} complete m8!")
    
    y_pred = rfc.predict(X_test)
    
    if print_results:
        print("I am confusion:")
        print(confusion_matrix(y_test, y_pred))
        print("\nClassification Report:")
        print(classification_report(y_test, y_pred))
        
    if returns_report:
        return classification_report(y_test, y_pred)
    

def plot_metrics_at_ks(metrics, data, savefig=False):

    """
    Plots the linechart of a evaluation metric of each class (punch type)

    (This function is designed to plot for the KNN model. If needed, will be modified later for general cases)

    :param str metrics: The metric to plot. Select between: "Accuracy", "Precision", "Recall", "F1-score"
    :param DataFrame data: A DataFrame of the given evaluation metric of each class.
        (Note: We need to generate this DataFrame manually)
    :param bool savefig (optional): Save the figure in local. False by default.
        (if True, please maunally adjust the figure name and the path accoringly to your local path)

    :return: A linechart of a evaluation metric of each class.
    """
    if metrics == 'Accuracy':
        accuracy_x = list(range(1, 101))
        plt.plot(accuracy_x, data)

        plt.xlabel("k-neighbor")
        plt.ylabel("accuracy")
        plt.title("Overall accuracy at ks")

        if savefig:
            plt.savefig("Figures/Curve_accuracy_at_ks.png")

        plt.show()

    else:
        plt.figure(figsize=(12, 10))

        sns.lineplot(data)

        plt.xlabel("k-neighbour")
        plt.ylabel(f'{metrics}')
        plt.title(f"{metrics} of punch types at ks")

        if savefig:
            plt.savefig(f"Figures/Curve_{metrics}_at_ks.png")

        plt.show()


def aggregate_data(df, window):
    """
    Returns an aggregated dataframe on a given window size by the mean absolut value of each window.

        Discards the remainders that have less datapoints than the window size. Usually the 'tail' of the dataframe.
        Does not take the influence of time series into account. Usually applied for data preprocessing
            for a classical ML algorithm.

    :param df: The Raw, Unprocessed Data (collected by Phyphox or other device)
    :param int window: The window size of data aggregation.
    :returns df_agg: A dataframe with features aggregated by the mean absolut value of each window.
    """

    # Make a copy of the original df, remove the remaining window that are less than 50 (this can happen due to the device)
    len_df = len(df) // window * window
    df_copy = df[:len_df].copy()
    df_colnames = df_copy.columns

    # Make a template of aggregated df
    df_agg = {
        df_colnames[0]: [],
        df_colnames[1]: [],
        df_colnames[2]: [],
    }

    i = 0
    j = 50

    # Calculate the mean of absolute values of each window (every 50 datapoints)
    # All mean values will be rounded to 3 decimals
    while j <= len_df:
        df_window = df_copy[i:j]
        agg_values = []

        for col in df_window:
            agg_values.append(df_window[col].abs().mean())

        df_agg[df_colnames[0]].append(agg_values[0].round(3))
        df_agg[df_colnames[1]].append(agg_values[1].round(3))
        df_agg[df_colnames[2]].append(agg_values[2].round(3))

        i += 50
        j += 50

    # Convert aggregated values into a dataframe
    df_agg = pd.DataFrame(df_agg)

    return df_agg
